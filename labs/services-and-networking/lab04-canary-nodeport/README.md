# LAB 04 ‚Äì Canary Deployments with NodePort and Replica Weighting

## Deploying Applications the DevOps Way ‚Äì Canary Release (No Ingress)



---
## üéØ Lab Goal

Implement a **Canary Deployment** using **NodePort** and **replica weighting** to control traffic distribution.

---

## üìñ Related Comic
üëâ [comics/nodeport/01-canary-nodeport/README.md](../../../comics/nodeport/01-canary-nodeport/README.md)

---
## üìò Reference Docs

- Canary deployments ‚Üí [`docs/md-resources/canary-deployments.md`](../../../docs/md-resources/lab-canary-deployments-the-new-recipe-test.md)
- Introduction to Canary Deployments ‚Üí [Implementing Canary Deployments](../../../docs/md-resources/implementing-canary-deployments.md)
- Reference: Blue/Green vs Canary ‚Üí [Comparison](../../../docs/md-resources/related-deployment-strategies-comparison.md)
- Next Step: Advanced Traffic Splitting (Gateway API) ‚Üí [Advanced Traffic Splitting](../../../docs/md-resources/advanced-traffic-splitting.md)



---
## üìã Requirements

1. Run a Deployment named **oldbird**
   - Image: `nginx:1.18`
2. Run a Deployment named **newbird**
   - Image: `nginx:latest`
3. Expose both Deployments via **one NodePort Service**
4. Route traffic approximately:
   - **90% ‚Üí oldbird**
   - **10% ‚Üí newbird**

---

## üè¨ Mall Analogy

We are testing a **new shop layout** without rebuilding the mall entrance.

| Kubernetes Concept | Mall Analogy |
|-------------------|-------------|
| **oldbird Pods** | The trusted, old shop layout |
| **newbird Pods** | The experimental new layout |
| **Shared Service** | A single front door |
| **Replica count** | Number of shop assistants |
| **Traffic split** | Probability of who serves the customer |

Kubernetes doesn‚Äôt do percentages, it does **math with Pods**.

---

## üõ†Ô∏è Solution

### 1Ô∏è‚É£ Create the Base Deployment (oldbird)

```bash
kubectl create deploy oldbird \
  --image=nginx:1.18 \
  --dry-run=client -o yaml > old.yaml
```
Edit `old.yaml` and add a **shared label**:
```yaml
metadata:
  labels:
    type: bird
spec:
  template:
    metadata:
      labels:
        type: bird
```
Apply it:
```bash
kubectl apply -f old.yaml
```
### 2Ô∏è‚É£ Create the Canary Deployment (newbird)
Reuse the same file:
```bash 
vim old.yaml
```
Replace `oldbird` with `newbird` and `nginx:1.18` with `nginx:latest`:
Quick Vim helpers:
```vim
:%s/oldbird/newbird/g
:%s/nginx:1.18/nginx:latest/g
```
Apply it:
```bash
kubectl apply -f old.yaml
```
Verify:
```bash
kubectl get pods -l type=bird
```
### 3Ô∏è‚É£ Expose Both Deployments via One Service
Create a **NodePort Service** that selects **all bird pods**:
```bash
kubectl expose deploy oldbird \
  --name=bird \
  --port=80 \
  --selector=type=bird \
  --type=NodePort
```
Check the service:
```bash
kubectl get svc bird
```
### 4Ô∏è‚É£ Observe Traffic Distribution (Initial State)
```bash
curl $(minikube ip):<NODEPORT>
```
At this point:

- oldbird = 1 pod
- newbird = 1 pod
‚Üí ~ **50/50 traffic**

### 5Ô∏è‚É£ Shift Traffic to 90/10 (The Canary Step)

Scale only the old Deployment:
```bash
kubectl scale deploy oldbird --replicas=9
```
Check the pods:
```bash
kubectl get pods -l type=bird
```
At this point:

- oldbird = 9 pods
- newbird = 1 pod

  ‚Üí ~ **90/10 traffic split**

Verify again:
```bash
kubectl describe svc bird
```
üîé **How Traffic Is Really Split**

Kubernetes Services do:

- Round-robin across Endpoints
- No weights
- No intelligence
- Traffic ratio = **number of Pods**

üß† **Expert Summary**

- Canary deployments can be implemented without Ingress
- Replica count = traffic percentage
- NodePort + selector = shared entry point
- This is a **low-level**, **exam-friendly pattern**

üìù **CKAD Exam Tips**

- Expect **NodePort-based canaries**
- Ingress-based canaries are often out of scope
- Know how Services select Pods
- Always check `describe svc`‚Üí Endpoints
